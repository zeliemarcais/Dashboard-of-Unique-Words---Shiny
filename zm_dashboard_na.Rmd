---
title: "Development of a learning Shiny app/dashboard"
author: "Zelie Marcais"
date: "Start: 2021-02-11 || Expected finish: 2021-03-11"
output:
    rmdformats::readthedown
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rmdformats)
library(readxl)                        
eunioa_df <- read_excel("eunioa_df.xlsx")
View(eunioa_df)

```

> The goal of science is to make the wonderful and complex understandable and simple---but not less wonderfull <br /> --- Herbert Simon

# Idea 1: Languages - Unique words
## Description
## Workflow
1.  *Data*
-   Scraping words, definitions, languages and tags from [Eunoia](https://eunoia.world/)
-   Google Trends. Use **gtrendsR** package
-   Geographical data to map
2.  *Wrangling*
-   Build data frame of words
-   Join word frequency use fro Google Trends
-   2.3.) Map word frequency use by country
3.  *Visualisation*
-   Wordcloud of unique word frequency of use
-   Map frecuency
4.  *Dashboard*
-   Data
-   Tab 1: Wordcloud
-   Tab 2: Leaflet map

## Timeline
Schedule to develop the app in 1 month:
```{r timeline, results='asis',echo=FALSE, message=FALSE}

library(knitr)
library(tidyverse)

# Tasks/Goals
knitr::kable(tibble(week_1 = list("Planning","Choice of app","Packages"), 
                    week_2 = list("Data and Wrangling","App layout","How to publish app in shiny.io"),
                    week_3 = list("1st version of dashboard","Map building","Leaflet and ggmaps"),
                    week_4 = list("Final version","Testing","Publishing online")), caption="Tasks and goals")

```

## Toolkit: Things needed to build it
-   [Mastering Shiny](https://mastering-shiny.org/)
-   [Geocomputation](https://geocompr.robinlovelace.net/)
-   [R Markdown](https://bookdown.org/yihui/rmarkdown/)
-   [Blogdown](https://bookdown.org/yihui/blogdown/)
-   [Tidytext](https://www.tidytextmining.com/)
-   [Leaflet](https://rstudio.github.io/leaflet/)
-   GitHub
-   Wordcloud

# SCRAP

```{r eunoia, results='asis'}

library(rvest)
library(tidyverse)
library(readxl)
library(writexl)

# URL  ####
eunoia_url <- "https://eunoia.world/"

# Step 1 - Create an empty list and an empty data frame. We will store the results there ####

eunoia <- list()
eunoia_df <- tibble()

# Step 2 - We'll use while function to scrap the website until we have 500 different words. Running it multiple times I've found 530 different words max after running this several times. How can we find the total number of distinct words in a more analytic way? ####

while (tally(eunoia_df) <= 530) {
  
  # i) Loop 50 scraps of the website and save them into the empty list
  for (i in c(1:50)) {
  
  # A - Start with empty list and double bracket to save as elements
  eunoia[[i]] <- eunoia_url %>%
    # Make RL into readable HTML
    read_html() %>%
    # Extract all table on each query
    html_nodes("td:nth-child(3) , td:nth-child(2) , td:nth-child(1)") %>%
    # Convert to text and then tibble
    html_text() %>%
    as_tibble() %>%
    # Create ID for each word and label each object
    dplyr::mutate(names = rep(c("word","description","language"), times = 30),
                id = rep(1:30, each=3)) %>%
    # Pivot the table to have one column for words, other for descriptions and another for languages
    tidyr::pivot_wider(values_from = value, names_from = names)

  # B - Put together all the results into a data frame using map_df and removing repeated words
  eunoia_df <- eunoia %>%
  purrr::map_df(bind_rows) %>%
  dplyr::distinct(word,.keep_all = TRUE) %>%
  dplyr::mutate(word2 = str_to_lower(word),
                # ERROR - takes only first word and ruins expressions, specially in french
                word = word2 %>% word(1))
  
  }}
  
  view(eunoia_df)

#write.xlsx(eunoia_df, file = "/Volumes/GoogleDrive/My Drive/eunioa_df.xlsx", col.names = TRUE, append = FALSE)

```

# GOOGLE TRENDS

```{r googletrends}

library(gtrendsR)      # Google Trends
library(tidyverse)     # All tidyverse tools

# 1. Create an empty list were we will save the results for each word
eunoia_gtrends <- list()

# 2. Bring all Google Trends results for the last year for every word - making a loop 
for (i in eunoia_df$word) {
 eunoia_gtrends[[i]] <- gtrends(keyword = print(i), time = "all")
}

# 3. Pluck the hits for each word, saving them into an empty list
gtrends_list <- list()
for(i in 1:length(eunoia_gtrends)){
  gtrends_list[[i]] <- purrr:: pluck(eunoia_gtrends,i, 2) %>%
    # Make all hits variables numeric
    dplyr::mutate(hits = as.numeric(hits))
  }

# 4. Make the plucked data into a data frame using map_df
gtrends_df <- gtrends_list %>%
  # Bind rows for all results
  purrr::map_df(bind_rows) %>%
  # Drop NA values
  tidyr::drop_na(hits) %>%
  # Keep only what we want
  dplyr::select(word=keyword,location,hits)

```

# MAPPING

```{r maptest, eval=FALSE}

library(gtrendsR)      # Google Trends
library(leaflet)       # Leaflet map
library(rgeos)
library(rworldmap)
library(sf)
library(sp)
library(spData)
library(raster)        # Maps
library(tigris)
library(rnaturalearth)
library(RColorBrewer)
library(shiny)
library(shinydashboard)
library(ggthemes)
library(tidyverse)     # All tidyverse tools


# 1.Static map ####
gtrends_df %>%
  dplyr::left_join(spData::world %>%
                     dplyr::select(name_long,geom), by=c("location"="name_long")) %>%
  ggplot()+
  geom_sf(aes(fill=hits,geometry = geom))

# 2. Leaflet ####
gtrends_df %>%
  dplyr::left_join(rworldmap::getMap(resolution="low") %>%
                     rgeos::gCentroid(byid=TRUE) %>%
                     as.data.frame() %>%
                     tibble::rownames_to_column("country") %>%
                     dplyr::rename("long"=x,"lat"=y), by=c("location"="country")) %>%
  leaflet() %>%
  setView(zoom = 1) %>%      
  addTiles() %>%
  addProviderTiles(providers$Stamen.Watercolor) %>%
  addMarkers(label= ~paste0(location,": ",hits))

# 3. Leaflet polygons ####
 ## tigris package to join geo data with words (with reactiveness)
carte <- tigris::geo_join(rnaturalearth::ne_countries(),
                          gtrends_df %>%
                            dplyr::filter(language==print(input$language)), 
                          by_sp = "admin",
                          by_df = "location",
                          how = "left")

carte %>% 
  leaflet() %>%
  setView(zoom = 1) %>%      
  addTiles() %>%
  addProviderTiles(providers$Stamen.Watercolor) %>%
  ##polygon data with reactive function
  addPolygons(data = carte@data %>% dplyr::filter(word=="te"),
                    fillColor = ~pal(hits),
                    color = "grey80",
                    fillOpacity = 0.7,
                    highlight = highlightOptions(
                      color = "black",
                      fillOpacity = 0.4,
                      bringToFront = TRUE),
                    label= ~paste0(location,": ",hits))

```

# SENTIMENT $PENDING$ 

```{r sentiment}

sentiments_df <- read_excel("eunioa_df_sentiments.xlsx")
View(sentiments_df)

```

# DASHBOARD 


```{r dashboard}

# Packages ####
library(gtrendsR)      # Google Trends
library(leaflet)       # Leaflet map
library(rgeos)
library(rworldmap)
library(sf)
library(sp)
library(spData)
library(raster)        # Maps
library(tigris)
library(RColorBrewer)
library(shiny)
library(shinydashboard)
library(shinyWidgets)
library(ggthemes)
library(DT)
library(tidyverse) 
library(plotly)

# UI ####
ui_zm <- dashboardPage(
  
  dashboardHeader(title = "An Untranslatable Words Study, by Zelie Marcais", titleWidth = 500),
  skin = "yellow", 
  
  dashboardSidebar(
    sidebarMenu(
    
    # Data tab    
    menuItem("Data", tabName = "data", icon = icon("database")),
    
    # Map tab
    menuItem("Map", tabName = "map", icon = icon("globe-americas")),
    
    # Sentiment tab
    menuItem("Sentiment", tabName = "sentiment", icon = icon("surprise"))
    
    ))
  ,
  
  dashboardBody(
    
    # Data tab ####
    tabItems(
      tabItem(tabName = "data",
              h2("The Database"),
              # Box on the left 
              fluidRow(box(title = "Unique words", "To study this subject, I will be using this dataframe of untranslatable words, scrapped from the website", a("Eunoia World."), href = "https://www.eunoia.world/",
                           "To scrape this website and extract the data, since this website randomly presents 30 words each time the page is loaded, I created a loop that would extract up to 530 words, keeping only unique observations. See the code below for more details.", width = 24, collapsible = TRUE)),
              
              #box on the top right
        fluidRow(
          box(div(style = 'overflow-y:scroll;height:200px;',
                  h4("Scrap Code"), 
                br(), 
              includeMarkdown("/Volumes/GoogleDrive/.shortcut-targets-by-id/14FXDX5iN2UM1M5_0DzCD74GIzDIaV3Yz/2021_zelie_marcais/scrap_code.R"),  
                  collapsible = TRUE, collapsed = TRUE,
                  width = NULL)),
          
             
              #box bottom left 
                 box(selectInput(
                inputId = "alllanguages", 
                label = "Select language",
                choices = unique(eunoia_df$language),
                selected = "English"))),
              

              #box bottom right
               fluidRow(box(title = "Browse unique Words and Languages", dataTableOutput("table"), width = 24),
      )),
            
    # Map tab #### ADD BAR CHART TO SIDE ? 
    tabItem(tabName = "map",
            h2("Explore the Geography of Unique Words"),
            # Box on the left - Input
             fluidRow(box(selectInput(
                  inputId = "allwordgeos",
                 label ="Select word below:",
                  choices = eunoia_df %>% 
                    dplyr::select(word) %>%
                    dplyr::arrange(word),
                  selected = "vergÃ¼enza")),
                 
            uiOutput("worddefinition"), width =6, height = 350),
    
                #second  box - map 
              fluidRow(box(Title = "Geographical Distribution of Unique Words", 
                             leafletOutput("map"), width = 24)) 
                           #box(title = "Distribution", plotlyOutput("graph2"))
                
    ),
    
    # Sentiment tab ####
    tabItem(tabName = "sentiment",
            fluidRow(
            h2("The Sentiment of Languages"),
            box(title = "Sentiment Analysis", "I've imputed sentiments (14) to each of the words from the database. with this, you can explore the distribution of sentiments in unique words of each langauge, which demonstrates the interconnectedness between language and culture."),
            box(title = "Disclaimer", " I have classified these words myself, from my own interpretation to best of my ability. There is a large marge of subjectivity"), 
            box(title = "Select language", 
                selectInput(
                  inputId = "sentimentlang",
                  label= "Select language below:",
                  choices = sentiments_df %>% 
                    dplyr::select(language) %>%
                    dplyr::arrange(language),
                  selected = "English")),
             box(title="Top Sentiments by Language", plotlyOutput("graph1"), width = 12))
            )
            
    ) #tab items
    ) #dashboard body 
  ) #dashboard page 



##### Server ####
server_zm <- function(input, output) {
  
#REACTIVE FUNCTIONS  
  
  # Data reactive for DT#
data_df <- reactive({
      b <- subset(eunoia_df, language == input$alllanguages)
    return(b)
  })

  # Reactive data for map - Polygons - Using TIGRIS package to join polygon data with word df 
  data_poly <- reactive({
    
    tigris::geo_join(rnaturalearth::ne_countries(),
                     gtrends_df %>%
                            dplyr::filter(word==print(input$allwordgeos)),
                          by_sp = "admin",
                          by_df = "location",
                          how = "left")#%>%
        #purrr::modify(hits, ifelse(is.na(.), 0, .))
  })
   
#definition reactive ### need to edit the output to seperate the 3 elements ? 
word_definition <- reactive({
  eunoia_df %>% dplyr::select(description, word, language)%>%
    filter(word == input$allwordgeos)
  
})

#sentiment reactive 
data_stmt <- reactive({
    filter(sentiments_df, language %in% input$sentimentlang)
  })


# Leaflet color palette
  pal <- leaflet::colorNumeric(palette = "Greens", domain = gtrends_df$hits)
  
  #erase colors for NA values 
  factop <- function(x) {
  ifelse(is.na(x), 0, 1)
}
  
# Leaflet map #### ADD DISPLAY OF input word DEFINITION ? 
  output$map <- renderLeaflet({
    
    leaflet() %>%  
      setView(lat = 30, lng = -40, zoom = 1) %>%
      addProviderTiles(providers$CartoDB.Positron)
    
  })
  
  observe({
    
    leafletProxy("map") %>%
        clearShapes() %>%
        clearControls() %>%
        clearMarkers() %>%
        addPolygons(data = data_poly(),
                    stroke = FALSE, 
                    smoothFactor = 0.2, 
                    fillOpacity = ~factop(hits),
                    fillColor = ~pal(hits),
                    color = "grey80",
                    highlight = highlightOptions(
                      color = "black",
                      fillOpacity = 0.4,
                      bringToFront = TRUE),
                    label= ~paste0(location,": ",hits)) %>%
        addLegend(data = data_poly(),
                  pal = pal, 
            values = ~hits, 
            opacity = 0.7, 
            position = "topright",
            title = "Google Trends Hits")
    })
  
 #output infobox definition  
  output$worddefinition <- renderUI({
    infoBox(unique(input$allwordgeos),
      value =tags$p(HTML(paste(word_definition())),
            style = "font-size: 90%"),
            width = 6,
            color = "light-blue",
            fill = FALSE)
            })
  
  # DT table 
  output$table <- renderDataTable(data_df())
  
  ##sentiment graph # 
  output$graph1 <- renderPlotly({
    
        p <- data_stmt() %>%
          dplyr::group_by(sentiment) %>%
          dplyr::count() %>%
          dplyr::ungroup() %>%
          ggplot(aes(x=sentiment,y=n, fill=sentiment))+ 
          ###ADD Color 
       geom_col(stat="identity", position= "stack", width = .5)+
          labs(x=NULL,y=NULL)+
          coord_flip()+
          theme_classic()
      
        # Ggplotly
        ggplotly(p) %>%
          layout(showlegend=FALSE)
      
      })
}

# Run App ####
shinyApp(ui=ui_zm,server=server_zm)

```

# HOMEWORKS 

```{r}

library(WDI)
library(sf)
library(spData)
library(raster)
library(RColorBrewer)
library(ggthemes)
library(tidyverse)


spData::world %>%
  dplyr::left_join(WDI(country = "all", indicator="AG.LND.FRST.K2") %>%
                     dplyr::select(iso2c,country,forest=3,year) %>%
                     dplyr::filter(year==2016), by=c("iso_a2"="iso2c")) %>%
  dplyr::filter(!str_detect(country,"Antarc")) %>%
  ggplot()+
  geom_sf(aes(fill=log(forest)))+
  scale_fill_distiller(palette="Greens", direction = 1)+
  theme_map()

```

# LEAFLET 
```{r}
library(leaflet)
library(raster)
library(RColorBrewer)
library(usmap)
library(tidyverse)

us <- raster::getData(name = "GADM", country = "USA", level=2)

us_state_counties <- usmap::countypop %>%
  dplyr::left_join(usmap::statepop, by="abbr") %>%
  dplyr::rename("state"="full")

california <- us %>%  
  ggplot2::fortify(region="NAME_2") %>%
  dplyr::right_join(us_state_counties %>% 
                     dplyr::filter(state=="California"), by=c("id"="county"))


  dplyr::group_by(id) %>%
  dplyr::mutate(poisson = rpois(n(),1)) %>%
  dplyr::ungroup()

#Markers
df_exercise_2 %>%
  leaflet() %>%
  addTiles() %>%
  addPolygons(lng=~long,
             lat=~lat)

df_exercise_2 %>%
  dplyr::distinct(id,.keep_all = TRUE) %>%
  dplyr::select(poisson) %>%
  ggplot()+
  geom_histogram(aes(x=poisson))

# Choropleths
bins <- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)
palette <- colorBin("Spectral", domain = df_exercise_2$poisson, bins = bins)
  
addPolygons(
  fillColor = ~palette(poisson),
  weight = 2,
  opacity = 1,
  color = "white",
  dashArray = "3",
  fillOpacity = 0.7)

```

# Idea 2: Ecology/Plants


##notes 

  # Reactive data for map - Markers
  data_markers <- reactive({
    
    gtrends_df %>%
        dplyr::filter(word==print(input$word)) %>%
        dplyr::left_join(rworldmap::getMap(resolution="low") %>%
                     rgeos::gCentroid(byid=TRUE) %>%
                     as.data.frame() %>%
                     tibble::rownames_to_column("country") %>%
                     dplyr::rename("long"=x,"lat"=y), by=c("location"="country"))
  })

